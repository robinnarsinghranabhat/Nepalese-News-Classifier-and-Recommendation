{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN word2vec Model on lots of Nepalese text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "#load the data on which model is to be trained\n",
    "datas = pd.read_csv('goodNewsData')\n",
    "\n",
    "#the cleanText column contain all news stemmed ,\n",
    "#the noStopWords column contain the news with stopWords remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allDirectories = ['World','National News','Auto','Entertainment','Sports','Education','Bank','Business Interview','Interview','Blog','Literature','Employment','Opinion','Technology','Tourism','Economy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dont run this block if you just want to use word2vec\n",
    "labeldataset = datas['labels']\n",
    "labelTrain = labeldataset[0:12500]\n",
    "labelTest = labeldataset[12500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read stopwords from directory and convert it into list \n",
    "stopwordsNep = u\"\"\" \"\"\"\n",
    "with codecs.open('stopwordsNepali.txt','r','utf-8') as file:\n",
    "    stopwordsNep += file.read()\n",
    "stopwordsNep = re.sub(r'(\\n)',\" \",stopwordsNep).strip(' ')\n",
    "\n",
    "stopwordsList = set(stopwordsNep.split(' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if already trained , don't run this code \n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "import multiprocessing\n",
    "doc1 = datas['noStopWords']\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in zip(datas.index.tolist(),doc1):\n",
    "    words = text.split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "\n",
    "nxtmodel = doc2vec.Doc2Vec(docs, size = 100, window = 10, min_count = 20, workers = multiprocessing.cpu_count(),seed= 42)\n",
    "nxtmodel.save('w2vModelnxt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model we just trained and saved\n",
    "mymodel = doc2vec.Doc2Vec.load('w2vModelnxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Results of training are shown below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('मतगणना', 0.7981842756271362),\n",
       " ('मतपत्र', 0.7580563426017761),\n",
       " ('उम्मेदवार', 0.7516075372695923),\n",
       " ('मनोनित', 0.7416603565216064),\n",
       " ('निर्वाचन', 0.7273353338241577),\n",
       " ('बहुमत', 0.7193455696105957),\n",
       " ('मत', 0.7061256170272827),\n",
       " ('उम्मेदवारी', 0.6930205225944519),\n",
       " ('निर्वाचित', 0.6832913160324097),\n",
       " ('चुनाव', 0.6825754642486572)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.most_similar(('मतदान'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('एमालेकै', 0.6447604894638062),\n",
       " ('निकट', 0.6222361326217651),\n",
       " ('नेता-कार्यकर्ता', 0.6206057667732239),\n",
       " ('पूर्वप्रधानमन्त्री', 0.6181797981262207),\n",
       " ('झलनाथ', 0.604692816734314),\n",
       " ('सत्तारुढ', 0.598918616771698),\n",
       " ('कार्यकर्ता', 0.595262348651886),\n",
       " ('एमालेभित्र', 0.5867376327514648),\n",
       " ('मधेसवादी', 0.5675921440124512),\n",
       " ('भाजपा', 0.5623850226402283)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.most_similar(('नेता'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('चैत', 0.9012683629989624),\n",
       " ('बैशाख', 0.8877366781234741),\n",
       " ('फागुन', 0.8769859075546265),\n",
       " ('साउन', 0.8699873685836792),\n",
       " ('भदौ', 0.868495762348175),\n",
       " ('असोज', 0.8654237985610962),\n",
       " ('असार', 0.8625174760818481),\n",
       " ('माघ', 0.860461413860321),\n",
       " ('मंसिर', 0.8565883636474609),\n",
       " ('पुस', 0.8451778888702393)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#that's my birthday month .\n",
    "mymodel.most_similar(('जेठ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('पाल्पा', 0.8077436685562134),\n",
       " ('रत्ननगर', 0.7850657105445862),\n",
       " ('कास्की', 0.7808139324188232),\n",
       " ('ललितपुर', 0.7694480419158936),\n",
       " ('भरतपुर', 0.7647161483764648),\n",
       " ('बर्दिया', 0.7613236308097839),\n",
       " ('तनहुँ', 0.7589349150657654),\n",
       " ('अर्घाखाँची', 0.7520005702972412),\n",
       " ('धनकुटा', 0.7476468086242676),\n",
       " ('भक्तपुर', 0.7475557923316956),\n",
       " ('निकुञ्ज', 0.7375757694244385),\n",
       " ('बैतडी', 0.7299805283546448),\n",
       " ('डडेल्धुरा', 0.7285469770431519),\n",
       " ('बाजुरा', 0.7252779603004456),\n",
       " ('डडेलधुरा', 0.7235883474349976),\n",
       " ('पर्वत', 0.7206146121025085),\n",
       " ('बझाङ', 0.7193875312805176),\n",
       " ('इलाम', 0.7171655893325806),\n",
       " ('मकवानपुर', 0.7152278423309326),\n",
       " ('कालिकोट', 0.7141162157058716),\n",
       " ('सुर्खेत', 0.7094873189926147),\n",
       " ('नेपालगन्ज', 0.7080104947090149),\n",
       " ('बागलुङ', 0.7078388929367065),\n",
       " ('जुम्ला', 0.7052714824676514),\n",
       " ('सुदूरपश्चिम', 0.7004926800727844),\n",
       " ('लेखनाथ', 0.6977355480194092),\n",
       " ('पाँचथर', 0.6972610950469971),\n",
       " ('डोटी', 0.6934576034545898),\n",
       " ('स्याङ्जा', 0.6908113956451416),\n",
       " ('पाटन', 0.6878213882446289)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my hometown , how could i not add this name\n",
    "mymodel.most_similar(('चितवन'),topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dont worry about this block , it's just regular expression's I used to clean the code\n",
    "\n",
    "# #this will hold list of news , but they will be cleaned .. just run cell below and see the result\n",
    "# convertedCleaned = []\n",
    "# #lines below remove things like ले , को , मा e.t.c from our dataset\n",
    "\n",
    "# for paragraph in datasText:\n",
    "# #     cleanParagraph = paragraph.replace(',',\" \")\n",
    "# #     cleanParagraph = re.sub(r'([a-z]+)(\\n+)|\\\"*(\\n+)|\\'*(\\n+) | \\\\*[a-z]*\\s+',\" \",cleanParagraph,flags=re.IGNORECASE )\n",
    "# #     cleanParagraph = cleanParagraph.replace('।','')\n",
    "# #     convertedCleaned.append(re.sub(r'ले{1}\\s*|को{1}\\s*|मा{1}\\s|बाट{1}\\s*|लाई{1}\\s*|का{1}\\s*',\" \",cleanParagraph ))\n",
    "#     convertedCleaned.append(cleanNews(paragraph))\n",
    "# #remove whitespaces from front and back part of each news paragraph\n",
    "# for i in range(len(convertedCleaned)):\n",
    "    \n",
    "#     convertedCleaned[i] = convertedCleaned[i].strip()\n",
    "# # #this will hold list of news , but they will be cleaned .. just run cell below and see the result\n",
    "# convertedCleaned = []\n",
    "# #lines below remove things like ले , को , मा e.t.c from our dataset\n",
    "\n",
    "# for paragraph in getNews:\n",
    "#     cleanParagraph = paragraph.replace(',',\" \")\n",
    "#     cleanParagraph = re.sub(r'([a-z]+)(\\n+)|\\\"*(\\n+)|\\'*(\\n+) | \\\\*[a-z]*\\s+',\" \",cleanParagraph,flags=re.IGNORECASE )\n",
    "#     cleanParagraph = cleanParagraph.replace('।','')\n",
    "#     convertedCleaned.append(re.sub(r'\\W+ले{1}\\s+|\\W+को{1}\\s+|\\W+मा{1}\\s+|\\W+बाट{1}\\s+|\\W+लाई{1}\\s+|\\W+का{1}\\s+|\\W+हरु{1}\\s+|\\W+हरुसँग{1}\\s+|\\W+सँग{1}\\s+',\" \",cleanParagraph ))\n",
    "    \n",
    "# #remove whitespaces from front and back part of each news paragraph\n",
    "# for i in range(len(convertedCleaned)):\n",
    "    \n",
    "#     convertedCleaned[i] = convertedCleaned[i].strip()\n",
    "\n",
    "# #load and prepeare stopwords\n",
    "\n",
    "\n",
    "    \n",
    "# #load and prepeare stopwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
